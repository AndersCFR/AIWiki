{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6da39f04-fe0e-44e0-a627-8cfad30cc267",
   "metadata": {},
   "source": [
    "# <center> Fundamentos de Spark </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a869361-81aa-434c-a55a-4555f0b7bfdf",
   "metadata": {},
   "source": [
    "### Conceptos de Spark y Componentes\n",
    "\n",
    "Spark es un sistema de cómputo distribuido orientado al big data que trabaja en memoria y es independiente de otras tecnologías como Hadoop\n",
    "\n",
    "Spark está constituido por:\n",
    "\n",
    "El ecosistema de Spark incluye cinco componentes clave:\n",
    "\n",
    "1. Spark Core es un motor distribuido de uso general para procesar datos. En él se asientan las bibliotecas de SQL, procesamiento de streaming, aprendizaje automático y computación de grafos que puedes usar juntas en las aplicaciones. Este núcleo constituye la base de los proyectos y facilita el envío de tareas distribuidas, la programación y las funciones básicas de E/S.\n",
    "\n",
    "2. Spark SQL es el módulo que permite utilizar datos estructurados. Ofrece un método común para acceder a fuentes de datos diversas. Gracias a este módulo, puedes consultar datos estructurados de programas de Spark con SQL o con la API de DataFrame que te resulte más cómoda. Spark SQL admite la sintaxis de HiveQL y franquea el acceso a almacenes de Apache Hive. El modo de servidor proporciona conectividad estándar mediante JDBC u ODBC.\n",
    "\n",
    "3. Spark Streaming facilita la creación de soluciones de streaming escalables y tolerantes a fallos. Como incorpora la API con integración de lenguajes de Spark al procesamiento de streaming, puedes escribir tareas de streaming igual que lo haces con las tareas por lotes. Spark Streaming no solo admite Java, Scala y Python, sino que incluye semántica de una sola vez y con reconocimiento del estado que está lista para utilizarse.\n",
    "\n",
    "4. MLlib es la biblioteca escalable de aprendizaje automático de Spark. Contiene herramientas con las que las tareas prácticas de aprendizaje automático son sencillas y escalables, además de numerosos algoritmos de aprendizaje de uso habitual, como clasificación, regresión, recomendación y agrupación en clústeres. También incluye el flujo de trabajo y otras utilidades, como transformaciones de características, creación de flujos de procesamiento de aprendizaje automático, evaluación de modelos, álgebra lineal distribuida y estadísticas.\n",
    "\n",
    "5. GraphX es la API de Spark para grafos y computación en paralelo de grafos. Es flexible y funciona a la perfección tanto con grafos como con colecciones, de modo que unifica en un mismo sistema el proceso de extracción, transformación y carga (ETL), los análisis exploratorios y la computación iterativa de grafos. GraphX no es solo una API muy flexible, sino que también incluye varios algoritmos de grafos. Compite en rendimiento con los sistemas de grafos más rápidos, con la ventaja de que conserva la flexibilidad, la tolerancia a fallos y la facilidad de uso de Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731e77e3-4732-4acc-8839-3a6798a9f73e",
   "metadata": {},
   "source": [
    "### Creación de una sesión y context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb9b84de-7d3c-4f21-afb5-9313b4ff65b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://8959cc0189cb:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Fist app</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0xffff6b6b3e80>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Creación de una sesión\n",
    "# Dentro de local podemos especificar el número de cores, * sería para usar todos los cores disponibles\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"Fist app\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dda03fb7-6cfe-4e02-9bec-67c742a62e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación de un contexto\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d5a5e9-6ae2-4382-abb8-b91342dfd520",
   "metadata": {},
   "source": [
    "## RDD\n",
    "\n",
    "Resilient Distributed Dataset, es la principal abstracción de Spark\n",
    "\n",
    "* Dependencias: Es uma lista que indica cómo construir un proceso o dato\n",
    "* Particiones: Distribución a través de nodos, le da resiliencia\n",
    "* Función de cálculo: Genera iteradores para futuros cálculos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "932588dd-1163-4225-bf0d-9265876472f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación de un RDD vacío\n",
    "\n",
    "rdd_vacio = sc.emptyRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8998e1d-25ad-480a-8ed9-3fffaea6ac2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de particiones: 3\n"
     ]
    }
   ],
   "source": [
    "# Creación de un RDD vacío con 3 particiones usando parallelize\n",
    "\n",
    "rdd_vacio_3 = sc.parallelize([], 3)\n",
    "\n",
    "# Obtenemos el número de particiones\n",
    "print(\"Número de particiones:\", rdd_vacio_3.getNumPartitions())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
